{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVwxzWuQElka",
        "outputId": "58ce27a4-beb7-43af-888c-974f2b4ed643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++ mktemp -d\n",
            "+ cd /tmp/tmp.dZc5Uq5yxS\n",
            "+ wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run\n",
            "--2025-12-08 22:28:22--  https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.213.43.212, 23.213.43.205\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.213.43.212|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4245586997 (4.0G) [application/octet-stream]\n",
            "Saving to: ‘cuda_12.1.0_530.30.02_linux.run’\n",
            "\n",
            "cuda_12.1.0_530.30. 100%[===================>]   3.95G   279MB/s    in 15s     \n",
            "\n",
            "2025-12-08 22:28:38 (265 MB/s) - ‘cuda_12.1.0_530.30.02_linux.run’ saved [4245586997/4245586997]\n",
            "\n",
            "+ sudo sh cuda_12.1.0_530.30.02_linux.run --silent --toolkit\n",
            "+ rm cuda_12.1.0_530.30.02_linux.run\n",
            "NVIDIA (R) Nsight Compute Command Line Profiler\n",
            "Copyright (c) 2018-2024 NVIDIA Corporation\n",
            "Version 2024.2.1.0 (build 34372528) (public-release)\n"
          ]
        }
      ],
      "source": [
        "# CUDA 12.1 runfile，toolkit\n",
        "!set -x \\\n",
        " && cd $(mktemp -d) \\\n",
        " && wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run \\\n",
        " && sudo sh cuda_12.1.0_530.30.02_linux.run --silent --toolkit \\\n",
        " && rm cuda_12.1.0_530.30.02_linux.run\n",
        "\n",
        "import os\n",
        "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":/usr/local/cuda/bin\"\n",
        "\n",
        "!ncu --version\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!which nv-nsight-cu-cli || which ncu || echo \"nv-nsight-cu-cli / ncu not found\"\n",
        "!nv-nsight-cu-cli --version || ncu --version || echo \"nv-nsight-cu-cli / ncu\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLhfZaBuFKOQ",
        "outputId": "46d713e1-2822-441a-c9a7-4ed5347928c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda/bin/ncu\n",
            "/bin/bash: line 1: nv-nsight-cu-cli: command not found\n",
            "NVIDIA (R) Nsight Compute Command Line Profiler\n",
            "Copyright (c) 2018-2024 NVIDIA Corporation\n",
            "Version 2024.2.1.0 (build 34372528) (public-release)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDKA03z6F0Ym",
        "outputId": "5e2bf5f5-f19b-4a2c-a0e5-fe486e81f102"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  8 22:31:28 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   33C    P0             53W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"evaluate>=0.4.2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCtXd93BMOnK",
        "outputId": "8f8cc776-346d-4905-bbd6-c71598a61091"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "import os, time, json\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    Trainer, TrainingArguments, set_seed,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ------------ Basic device / GPU info ------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    cc_major, cc_minor = torch.cuda.get_device_capability(0)\n",
        "else:\n",
        "    gpu_name = \"CPU\"\n",
        "    cc_major, cc_minor = (0, 0)\n",
        "print(\"device:\", device)\n",
        "\n",
        "bf16_supported = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "print(f\"Device: {gpu_name}, CC: {cc_major}.{cc_minor}, bf16_supported={bf16_supported}\")\n",
        "\n",
        "# ------------ Config (keep original hyper-parameters; just add max_train_steps) ------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    dataset_id: str = \"dair-ai/emotion\"\n",
        "    model_name: str = \"bert-base-uncased\"\n",
        "    lr: float = 5e-5\n",
        "    per_device_batch_size: int = 8\n",
        "    num_epochs: int = 3\n",
        "    use_bf16: bool = bf16_supported\n",
        "    use_fp16: bool = (not bf16_supported) and torch.cuda.is_available()\n",
        "    weight_decay: float = 0.0\n",
        "    warmup_ratio: float = 0.0\n",
        "    grad_accum_steps: int = 1\n",
        "    seed: int = 42\n",
        "    output_dir: str = \"/content/bert_emotion_gpu\"\n",
        "    max_train_steps: int = 10  # run only 10 training steps for ncu profiling\n",
        "\n",
        "cfg = Config()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "# ------------ Dataset loading and tokenization ------------\n",
        "raw_ds = load_dataset(cfg.dataset_id)\n",
        "\n",
        "print(raw_ds)\n",
        "print(\"Train size:\", len(raw_ds[\"train\"]))\n",
        "print(\"Test size: \", len(raw_ds[\"test\"]))\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "tokenized = raw_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "train_ds = tokenized[\"train\"]\n",
        "eval_ds  = tokenized[\"test\"]\n",
        "\n",
        "num_labels = len(raw_ds[\"train\"].features[\"label\"].names)\n",
        "print(\"num_labels:\", num_labels)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    pad_to_multiple_of=None\n",
        ")\n",
        "\n",
        "metric_acc = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(-1)\n",
        "    return metric_acc.compute(predictions=preds, references=labels)\n",
        "\n",
        "# ------------ Model ------------\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    cfg.model_name, num_labels=num_labels\n",
        ").to(device)\n",
        "\n",
        "bf16 = cfg.use_bf16\n",
        "fp16 = cfg.use_fp16 and (not bf16)\n",
        "\n",
        "print(f\"Using bf16={bf16}, fp16={fp16}\")\n",
        "\n",
        "# ------------ Trainer / TrainingArguments (limit to 10 steps) ------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(cfg.output_dir, \"trainer_baseline\"),\n",
        "    per_device_train_batch_size=cfg.per_device_batch_size,\n",
        "    per_device_eval_batch_size=cfg.per_device_batch_size,\n",
        "    gradient_accumulation_steps=cfg.grad_accum_steps,\n",
        "    num_train_epochs=cfg.num_epochs,            # keep original epochs\n",
        "    max_steps=cfg.max_train_steps,              # explicitly limit to 10 steps\n",
        "    learning_rate=cfg.lr,\n",
        "    weight_decay=cfg.weight_decay,\n",
        "    warmup_ratio=cfg.warmup_ratio,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"no\",\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# ------------ Training + wall time measurement ------------\n",
        "t0 = time.perf_counter()\n",
        "train_result = trainer.train()\n",
        "wall_time = time.perf_counter() - t0  # measure total training wall time\n",
        "\n",
        "trainer.save_model()\n",
        "train_metrics = train_result.metrics\n",
        "print(\"train metrics:\", train_metrics)\n",
        "\n",
        "# # run one evaluation pass\n",
        "# eval_metrics = trainer.evaluate()\n",
        "# print(\"eval metrics:\", eval_metrics)\n",
        "\n",
        "# ------------ Throughput: samples/sec and tokens/sec ------------\n",
        "loader = DataLoader(train_ds, batch_size=cfg.per_device_batch_size, collate_fn=data_collator)\n",
        "\n",
        "total_tokens = 0\n",
        "total_samples = 0\n",
        "for batch in loader:\n",
        "    total_tokens += batch[\"attention_mask\"].sum().item()\n",
        "    total_samples += batch[\"input_ids\"].size(0)\n",
        "\n",
        "samples_per_sec = total_samples / wall_time\n",
        "tokens_per_sec  = total_tokens / wall_time\n",
        "\n",
        "print(f\"wall_time (s)   = {wall_time:.2f}\")\n",
        "print(f\"samples/sec     = {samples_per_sec:.2f}\")\n",
        "print(f\"tokens/sec      = {tokens_per_sec:.2f}\")\n",
        "\n",
        "summary = {\n",
        "    \"gpu_name\": gpu_name,\n",
        "    \"device_cc\": f\"{cc_major}.{cc_minor}\",\n",
        "    \"dtype\": \"bf16\" if bf16 else (\"fp16\" if fp16 else \"fp32\"),\n",
        "    \"train_samples\": total_samples,\n",
        "    \"train_tokens\": total_tokens,\n",
        "    \"wall_time_s\": wall_time,\n",
        "    \"samples_per_sec\": samples_per_sec,\n",
        "    \"tokens_per_sec\": tokens_per_sec,\n",
        "}\n",
        "\n",
        "print(json.dumps(summary, indent=2))\n",
        "\n",
        "out_path = os.path.join(cfg.output_dir, \"gpu_emotion_baseline_results.json\")\n",
        "with open(out_path, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"Saved to\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhSlgQVwMt4a",
        "outputId": "79cdc75f-8607-44f6-bcbe-de15f9363921"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdHvX6rZM6aM",
        "outputId": "43ab5a8d-9929-4c2a-afbe-bb7484d8e641"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 00:26:25.912241: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-09 00:26:25.930204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765239985.952699  327755 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765239985.959283  327755 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765239985.976925  327755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765239985.976965  327755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765239985.976969  327755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765239985.976973  327755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 00:26:25.982069: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "device: cuda\n",
            "Device: NVIDIA A100-SXM4-80GB, CC: 8.0, bf16_supported=True\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 16000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\n",
            "Train size: 16000\n",
            "Test size:  2000\n",
            "num_labels: 6\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using bf16=True, fp16=False\n",
            "/content/train.py:113: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "{'train_runtime': 0.5959, 'train_samples_per_second': 13.424, 'train_steps_per_second': 1.678, 'train_loss': 1.720703125, 'epoch': 0.0}\n",
            "100% 1/1 [00:00<00:00,  1.68it/s]\n",
            "train metrics: {'train_runtime': 0.5959, 'train_samples_per_second': 13.424, 'train_steps_per_second': 1.678, 'total_flos': 2104964038656.0, 'train_loss': 1.720703125, 'epoch': 0.0005}\n",
            "wall_time (s)   = 1.00\n",
            "samples/sec     = 15982.63\n",
            "tokens/sec      = 355765.42\n",
            "{\n",
            "  \"gpu_name\": \"NVIDIA A100-SXM4-80GB\",\n",
            "  \"device_cc\": \"8.0\",\n",
            "  \"dtype\": \"bf16\",\n",
            "  \"train_samples\": 16000,\n",
            "  \"train_tokens\": 356152,\n",
            "  \"wall_time_s\": 1.0010866170005102,\n",
            "  \"samples_per_sec\": 15982.632999270078,\n",
            "  \"tokens_per_sec\": 355765.4192472523\n",
            "}\n",
            "Saved to /content/bert_emotion_gpu/gpu_emotion_baseline_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed,gpu__time_duration.sum --log-file ncu_sm_1steps.csv --target-processes all python train.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUZJdEh_PHzP",
        "outputId": "f3941599-7c92-42cd-9ee5-7857872b744d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-09 01:29:24.274734: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-09 01:29:24.294514: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765243764.318042  374473 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765243764.324806  374473 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765243764.342064  374473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765243764.342112  374473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765243764.342115  374473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765243764.342117  374473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 01:29:24.347270: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "device: cuda\n",
            "Device: NVIDIA A100-SXM4-80GB, CC: 8.0, bf16_supported=True\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 16000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\n",
            "Train size: 16000\n",
            "Test size:  2000\n",
            "num_labels: 6\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using bf16=True, fp16=False\n",
            "/content/train.py:113: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "  0% 0/1 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "log_path = \"/content/ncu_sm_1steps.csv\"\n",
        "\n",
        "\n",
        "records = []\n",
        "\n",
        "with open(log_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "n = len(lines)\n",
        "i = 0\n",
        "\n",
        "while i < n:\n",
        "    line = lines[i]\n",
        "\n",
        "    # Look for the \"Command line profiler metrics\" section\n",
        "    if \"Section: Command line profiler metrics\" in line:\n",
        "        # ---- 1.1 Find the kernel header line above this section ----\n",
        "        j = i - 1\n",
        "        kernel_header = None\n",
        "\n",
        "        while j >= 0:\n",
        "            prev = lines[j].rstrip(\"\\n\")\n",
        "            stripped = prev.strip()\n",
        "\n",
        "            # Skip empty lines, separators, and other section headers\n",
        "            if (stripped == \"\" or\n",
        "                stripped.startswith(\"Section:\") or\n",
        "                stripped.startswith(\"Metric Name\") or\n",
        "                stripped.startswith(\"-\") or\n",
        "                stripped.startswith(\"==PROF==\")):\n",
        "                j -= 1\n",
        "                continue\n",
        "\n",
        "            # The first non-empty, non-==PROF==, non-table line\n",
        "            # just above the section is the kernel header.\n",
        "            kernel_header = stripped\n",
        "            break\n",
        "\n",
        "        if kernel_header is None:\n",
        "            # Could not find a header, skip this section\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # Short kernel name: everything before the first '('\n",
        "        kernel_name = kernel_header.split(\"(\")[0].strip()\n",
        "\n",
        "        # ---- 1.2 Parse the metric table rows under this section ----\n",
        "        metrics = {}\n",
        "        k = i + 1\n",
        "\n",
        "        while k < n:\n",
        "            l = lines[k].rstrip(\"\\n\")\n",
        "            stripped = l.strip()\n",
        "\n",
        "            # Empty line: end of this kernel's metric section\n",
        "            if stripped == \"\":\n",
        "                break\n",
        "\n",
        "            # New section or profiling header: also stop\n",
        "            if stripped.startswith(\"Section:\") or stripped.startswith(\"==PROF==\"):\n",
        "                break\n",
        "\n",
        "            # Skip table headers / separators\n",
        "            if stripped.startswith(\"-\") or stripped.startswith(\"Metric Name\"):\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            # Metric row example:\n",
        "            #   gpu__time_duration.sum                                    us        10.62\n",
        "            parts = stripped.split()\n",
        "            if len(parts) >= 3:\n",
        "                metric_name = parts[0]      # e.g. gpu__time_duration.sum\n",
        "                metric_unit = parts[-2]     # e.g. us\n",
        "                metric_value = parts[-1]    # e.g. 10.62\n",
        "                metrics[metric_name] = (metric_unit, metric_value)\n",
        "\n",
        "            k += 1\n",
        "\n",
        "        # ---- 1.3 Store record if both metrics are present ----\n",
        "        t_key = \"gpu__time_duration.sum\"\n",
        "        s_key = \"sm__throughput.avg.pct_of_peak_sustained_elapsed\"\n",
        "\n",
        "        if t_key in metrics and s_key in metrics:\n",
        "            t_unit, t_val_str = metrics[t_key]\n",
        "            s_unit, s_val_str = metrics[s_key]\n",
        "\n",
        "            try:\n",
        "                t_val = float(t_val_str)\n",
        "                s_val = float(s_val_str)\n",
        "            except ValueError:\n",
        "                i = k\n",
        "                continue\n",
        "\n",
        "            records.append({\n",
        "                \"kernel_header\": kernel_header,\n",
        "                \"kernel_name\": kernel_name,\n",
        "                \"time_value\": t_val,   # numerical time value (unit below)\n",
        "                \"time_unit\": t_unit,   # e.g. 'us'\n",
        "                \"sm_pct\": s_val,       # SM utilization %\n",
        "            })\n",
        "\n",
        "        # Continue after this section\n",
        "        i = k\n",
        "    else:\n",
        "        i += 1\n",
        "\n",
        "print(f\"Parsed {len(records)} metric records\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 2. Build a DataFrame and normalize time units\n",
        "# --------------------------------------------------------\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "if df.empty:\n",
        "    raise RuntimeError(\"No records parsed. Check log_path or metric names.\")\n",
        "\n",
        "# Map time unit to seconds\n",
        "unit_to_sec = {\"ns\": 1e-9, \"us\": 1e-6, \"ms\": 1e-3, \"s\": 1.0}\n",
        "df[\"time_sec\"] = df.apply(\n",
        "    lambda row: row[\"time_value\"] * unit_to_sec.get(row[\"time_unit\"], 1.0),\n",
        "    axis=1\n",
        ")\n",
        "df[\"time_ms\"] = df[\"time_sec\"] * 1e3\n",
        "\n",
        "print(\"Time units present:\", df[\"time_unit\"].value_counts().to_dict())\n",
        "print(f\"Total kernel time: {df['time_ms'].sum():.3f} ms\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 3. Global time-weighted average SM utilization\n",
        "# --------------------------------------------------------\n",
        "\n",
        "def time_weighted_avg_sm(df_in: pd.DataFrame) -> float:\n",
        "    \"\"\"Compute time-weighted average SM utilization (%) for a DataFrame subset.\"\"\"\n",
        "    t = df_in[\"time_sec\"]\n",
        "    s = df_in[\"sm_pct\"]\n",
        "    if (t > 0).sum() == 0:\n",
        "        return 0.0\n",
        "    return float((s * t).sum() / t.sum())\n",
        "\n",
        "global_sm = time_weighted_avg_sm(df)\n",
        "print(f\"\\nGlobal time-weighted SM utilization: {global_sm:.2f}%\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 4. Per-kernel aggregated stats (calls, total time, weighted SM)\n",
        "# --------------------------------------------------------\n",
        "\n",
        "def agg_kernel(group: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"Aggregate per-kernel statistics.\"\"\"\n",
        "    t = group[\"time_sec\"]\n",
        "    s = group[\"sm_pct\"]\n",
        "    return pd.Series({\n",
        "        \"calls\": len(group),\n",
        "        \"total_time_ms\": t.sum() * 1e3,\n",
        "        \"time_weighted_sm_pct\": float((s * t).sum() / t.sum()) if t.sum() > 0 else 0.0,\n",
        "    })\n",
        "\n",
        "kernel_stats = df.groupby(\"kernel_name\").apply(agg_kernel).reset_index()\n",
        "kernel_stats_sorted = kernel_stats.sort_values(\"total_time_ms\", ascending=False)\n",
        "\n",
        "print(\"\\nTop 20 kernels by total time:\")\n",
        "print(kernel_stats_sorted.head(20).to_string(index=False))\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 5. Classify kernels into high-level op types\n",
        "# --------------------------------------------------------\n",
        "\n",
        "def classify_kernel(name: str) -> str:\n",
        "    \"\"\"Heuristically classify kernel type based on its name.\"\"\"\n",
        "    lower = name.lower()\n",
        "    if \"gemm\" in lower:\n",
        "        return \"gemm\"\n",
        "    if \"fmha\" in lower or \"attention\" in lower:\n",
        "        return \"attention\"\n",
        "    if (\"layer_norm\" in lower or \"layernorm\" in lower or\n",
        "        \"gammabeta\" in lower or \"grad_input_kernel\" in lower):\n",
        "        return \"layernorm\"\n",
        "    if \"gelu\" in lower:\n",
        "        return \"gelu\"\n",
        "    if \"softmax\" in lower:\n",
        "        return \"softmax\"\n",
        "    if \"dropout\" in lower:\n",
        "        return \"dropout\"\n",
        "    if \"reduce_kernel\" in lower or \"reduceop\" in lower:\n",
        "        return \"reduce\"\n",
        "    if (\"copy_kernel\" in lower or \"direct_copy\" in lower or\n",
        "        \"bfloat16_copy\" in lower):\n",
        "        return \"copy\"\n",
        "    if \"elementwise_kernel\" in lower:\n",
        "        return \"elementwise\"\n",
        "    return \"other\"\n",
        "\n",
        "df[\"op_type\"] = df[\"kernel_name\"].apply(classify_kernel)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 6. Per-op-type time-weighted SM statistics\n",
        "# --------------------------------------------------------\n",
        "\n",
        "type_stats = []\n",
        "total_time_ms_all = df[\"time_ms\"].sum()\n",
        "\n",
        "for op_type, group in df.groupby(\"op_type\"):\n",
        "    t = group[\"time_sec\"]\n",
        "    s = group[\"sm_pct\"]\n",
        "    total_time_ms = float(t.sum() * 1e3)\n",
        "    if t.sum() > 0:\n",
        "        w_sm = float((s * t).sum() / t.sum())\n",
        "    else:\n",
        "        w_sm = 0.0\n",
        "    time_share = 100.0 * total_time_ms / total_time_ms_all\n",
        "\n",
        "    type_stats.append({\n",
        "        \"op_type\": op_type,\n",
        "        \"total_time_ms\": total_time_ms,\n",
        "        \"time_share_percent\": time_share,\n",
        "        \"time_weighted_sm_pct\": w_sm,\n",
        "        \"kernel_records\": len(group),\n",
        "    })\n",
        "\n",
        "type_stats_df = pd.DataFrame(type_stats).sort_values(\"total_time_ms\", ascending=False)\n",
        "\n",
        "print(\"\\nPer-op-type time-weighted SM stats:\")\n",
        "print(type_stats_df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6gLvWExjObG",
        "outputId": "7163b4d6-b9ac-41d7-85e5-e92177b3d733"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed 1700 metric records\n",
            "Time units present: {'us': 1700}\n",
            "Total kernel time: 41.091 ms\n",
            "\n",
            "Global time-weighted SM utilization: 39.87%\n",
            "\n",
            "Top 20 kernels by total time:\n",
            "                                                                                                                                                                                                                                                  kernel_name  calls  total_time_ms  time_weighted_sm_pct\n",
            "                                                                                                                                                                                                            fmha_cutlassB_bf16_aligned_64x64_k64_dropout_sm80   12.0        7.68727             30.781145\n",
            "                                                                                                                                                                                                                     fmha_cutlassF_bf16_aligned_64x64_rf_sm80   12.0        3.11259             50.323221\n",
            "                                                                                                                                                                                             ampere_bf16_s16816gemm_bf16_128x256_ldg8_relu_f2f_stages_64x3_tn   60.0        2.92892             63.477649\n",
            "                                                                                                                                                                                     void native::unrolled_elementwise_kernel<native::direct_copy_kernel_cuda  200.0        2.43922             30.150821\n",
            "                                                                                                                                                             void native::vectorized_elementwise_kernel<4, native::FillFunctor<float>, std::array<char *, 1>>  610.0        2.34183              2.672996\n",
            "void native::<unnamed>::multi_tensor_apply_kernel<native::<unnamed>::FusedOptimizerTensorListMetadata<4>, native::<unnamed>::FusedAdamMathFunctor<float, 4, 1, 0>, const float *, double, double, double, double, double, bool, const float *, const float *>   10.0        1.99975             40.507990\n",
            "                                                                                                                                                                                                   ampere_bf16_s16816gemm_bf16_128x64_ldg8_f2f_stages_64x4_nt   48.0        1.87317             49.598759\n",
            "                                                                                                                                                                              void native::vectorized_elementwise_kernel<4, native::bfloat16_copy_kernel_cuda  233.0        1.72774              3.879833\n",
            "                                                                                  void native::reduce_kernel<128, 4, native::ReduceOp<c10::BFloat16, native::func_wrapper_t<c10::BFloat16, native::sum_functor<c10::BFloat16, float, c10::BFloat16>::operator   73.0        1.62234             11.685734\n",
            "                                                                                                                                                                                                  ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_32x3_nn   48.0        1.55117             59.697719\n",
            "                                                                                                                                                                                             ampere_bf16_s16816gemm_bf16_128x128_ldg8_relu_f2f_stages_32x5_tn   12.0        1.37854             67.206460\n",
            "                                                                                                                                                                                                  ampere_bf16_s16816gemm_bf16_128x128_ldg8_f2f_stages_32x5_nn   12.0        1.31282             69.972811\n",
            "                                                                                                                                                                                                  ampere_bf16_s16816gemm_bf16_256x128_ldg8_f2f_stages_64x3_nt   12.0        1.28811             73.753523\n",
            "                                                                                                                                                                                                  ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_64x3_nt   12.0        1.27849             74.261954\n",
            "                                                                                                                                                                                                  ampere_bf16_s16816gemm_bf16_128x256_ldg8_f2f_stages_64x3_nn   12.0        1.18997             76.983172\n",
            "                                                                                                                                                         void native::vectorized_elementwise_kernel<4, native::CUDAFunctor_add<float>, std::array<char *, 3>>   50.0        1.00756              4.362771\n",
            "                                                                                                                                                              void native::<unnamed>::GammaBetaBackwardCUDAKernelTemplate<float, float, 32, 32, 256, 0, 1, 0>   25.0        0.89169              9.790405\n",
            "                                                                                                                                                                                        void native::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>   25.0        0.71554             59.923199\n",
            "                    void native::unrolled_elementwise_kernel<native::CUDAFunctor_add<float>, std::array<char *, 3>, 4, TrivialOffsetCalculator<2, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, memory::LoadWithCast<2>, memory::StoreWithCast<1>>   24.0        0.69169             47.127211\n",
            "                                                                                                                                                                             void native::<unnamed>::layer_norm_grad_input_kernel_vectorized<float, float, 0>   25.0        0.62596             32.441742\n",
            "\n",
            "Per-op-type time-weighted SM stats:\n",
            "    op_type  total_time_ms  time_share_percent  time_weighted_sm_pct  kernel_records\n",
            "       gemm       12.84769           31.266692             65.207020             222\n",
            "  attention       10.79986           26.283004             36.413299              24\n",
            "elementwise        4.52882           11.021531             11.156236             741\n",
            "       copy        4.19605           10.211688             19.369866             436\n",
            "      other        3.36643            8.192689             27.200164              72\n",
            "  layernorm        2.23319            5.434787             32.202672              75\n",
            "     reduce        1.66533            4.052819             11.501099              78\n",
            "       gelu        1.04767            2.549655             71.477890              24\n",
            "    dropout        0.39771            0.967884             45.257893              26\n",
            "    softmax        0.00791            0.019250              0.020000               2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-459317951.py:172: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  kernel_stats = df.groupby(\"kernel_name\").apply(agg_kernel).reset_index()\n"
          ]
        }
      ]
    }
  ]
}